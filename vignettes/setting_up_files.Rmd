---
title: "Preparing to use BarnebyLives!"
output: rmarkdown::html_vignette
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{Preparing to use BarnebyLives!}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
description: >
  How to download and configure taxonomic and geographic datasets required
  for a working BarnebyLives instance.
---

# Introduction 

```{r, echo = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

```{r setup, message = F, echo = FALSE, eval = F}
# remotes::install_github('sagesteppe/BarnebyLives')
library(BarnebyLives)
library(tidyverse)
library(sf)
library(terra)
```

This vignette walks you through five steps:  
  - 1) Download taxonomic data (5-15 minutes, ~500MiB)  
  - 2) Download geographic data (a couple hours depending on extent)  
  - 3) Set up the instance (30-45 minutes processing) 
  - 4) Verify success

System requirements:
  - Stable internet connection for downloads
  - 4-20 GB available disk space (varies by geographic extent)
  - R version â‰¥ 4.0.0

You only need to do this once before using the package for analyses or label printing.

#  Decide where to store data

BarnebyLives relies on local copies of several large datasets to minimize API calls and ensure consistent performance. Storage requirements vary by geographic scope:

  - Regional collections (single state/province): 4-8 GB
  - Multi-state collections (e.g., Great Plains): 8-12 GB
  - Continental collections (e.g., Michigan to California): 16+ GB

The data persistence approach offers several advantages:

  - Reduced dependency on internet connectivity during analyses
  - Consistent data versions across analyses
  - Faster processing compared to repeated API calls
  - Offline capability for field work

It's important that you decide where BarnebyLives will be installed on your computer. 
Below, I will be downloading and copying some of the data to an external drive mounted to my system, where I have a directory named 'Barneby_Lives-dev'. 

```{r}
# Example directory (adjust for your system)
bl_dir <- "/media/steppe/hdd/BL_sandbox"
```

Note: All file paths in analytical functions can be specified independently, allowing you to relocate data after setup without breaking functionality.

## Taxonomic Data

BarnebyLives uses the *World Checklist of Vascular Plants* as its primary taxonomic backbone. 
This continuously updated resource from Kew Gardens provides standardized nomenclature for global plant diversity and serves as the foundation for *Plants of the World Online (POWO)*.

The `TaxUnpack` function downloads and processes WCVP data, creating three lookup tables:

Complete species catalog
Infraspecific taxa (varieties and subspecies)
Genus-level taxonomy

```{r read in WCVP data, eval = F}
p2 <- file.path(bl_dir, 'taxdata-MI')
TaxUnpack(path = p2, bound = bound)
```

### IPNI 

Standard botanical author abbreviations from the International Plant Names Index (IPNI) ensure consistent citation formatting on specimen labels.

```{r Prepare material for authorship lookup tables, eval = F}
data('ipni_authors', package='BarnebyLives')
write.csv(ipni_authors, file.path(p2, 'ipni_author_abbreviations.csv'))
```

## Download all geographic data

### Automated downloads 

The `data_download` function retrieves most geographic datasets automatically. 

```{r data download geographic data, eval = F}
# I am using R markdown so want to be really sure save data to the correct location
# so I will use an absolute path
data_download(path = bl_dir) 
# if you are using an interactive R script. (file.R) you should be able to do something
# similar or even just cd into the directory where you want to save everything and 
# run the function ala `download-data()`
```

## Manual downloads
Geographic data are organized in tiles following standard conventions. 
Define your study area as a bounding box using decimal degrees (WGS84/EPSG:4326 - default systems in most cartography software are close enough). 

```{r determine tiles to download, eval = F}
bound <- data.frame(
   y = c( 48,  48,  41,  41,  48),
   x = c(-91, -82, -82, -91, -91)
 ) # note that the 5th pair of coordinates is the same
# as the first pair (48, -91), this is required to 'close'
# the square. 

tileSelector(bound)
```

`tile_selector` will help you determine which files you need to download. 
We will define a geographic extent that covers the area of interest. 
It will be a simple square, and grabbing coordinates from Open Street/Google/Apple Maps etc., or a GIS, will suffice.

### manual 

We will download a couple attributes from Geomorpho90m from a [Minio](https://opentopography.s3.sdsc.edu/minio/dataspace/OTDS.012020.4326.1/raster/) page. 
We need both '*aspect*' (NOT 'sine' or 'cosine') and '*geom*' from here, you should be able to control find ('ctrl+'f') these in their respective directories and manually download them.  
Once the files are downloaded copy them to the same location as you had `data_download` save files. 

Now we also need to manually download data on elevation from [MERIT-DEM](https://hydro.iis.u-tokyo.ac.jp/~yamadai/MERIT_DEM/).
Yes you technically need to register here, but you will automatically get a log in password to any email you enter, and it works!

These tiles follow the same naming convention as the previous data set - in fact Geomorpho90m is derived from MERIT-DEM one. 
Also save these files to the location you have `data_download` save files. 

## set up an instance for use 

You are now ready to set up an instance of BL for analysis. 

Specify where the downloaded data are `path`, where to save the date `pathOut`, the square (or bounding box) around your study area `bound`, and whether to remove the raw downloaded files 'cleanup'. 
I set `cleanup = FALSE`, until I am 100% certain that all downloads are good, which means running data through the pipeline and printing labels a couple times. 

```{r, eval = F}
data_setup( # this one will take maybe 30 minutes or so 
  path = bl_dir, 
  pathOut = file.path(bl_dir, 'geodata-MI'),
  bound = bound, cleanup = FALSE
  )
```

# Finishing up. 

And that's it!
After a few runs of the system feel free to delete the raw data, you can do that by hand, or by initializing the instance using the `data_setup` function again. But that kind of seems like overkill!


## Next Steps
With your BarnebyLives instance successfully configured, you can now:

- Validate taxonomic names against WCVP standards
- Extract environmental data for specimen localities

Consult the additional BarnebyLives vignettes for analysis workflows and advanced usage examples.
