---
title: "setting up files"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{setting up files}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, eval = F}
library(BarnebyLives)
library(tidyverse)
```

We are hoping to have a cloud instance of BarnebyLives(!) running soon. 
This is able to serve persons which have queries for collection in the Continental United States portions of Western North America (~CONUS).
Essentially, you should be able to submit a query, and we will process it and send it back. 

For collectors operating in smaller geographic areas or herbaria with PC's, you are able to readily create your own local instance of BarnebyLives(!) quite easily. 

## Taxonomic portion

**Worldwide Checklist of Vascular Plants** provides the local taxonomic information required to perform much of the name matching.
We utilize it to ensure that queries sent to **Plants of The World Online** are spelled correctly, in order to reduce the number of calls to it. 
"The World Checklist of Vascular Plants, a continuously updated resource for exploring global plant diversity" serves as the basis for Kews Plants of the Worlds, this publication details the process behind the data set. 
We can download a static copy of the data from Kews data portal at: http://sftp.kew.org/pub/data-repositories/WCVP/. It is not that large of a file, so it should come over in a couple minutes. 

```{r read in WCVP data, eval = F}

p2 <- '~/Documents/assoRted/Barneby_Lives_dev/wcvp'

names <- read.table(file.path(p2, "wcvp_names.csv"), sep="|", header=TRUE, 
                    quote = "", fill=TRUE, na.strings = "", encoding = "UTF-8") 
distributions <- read.table(file.path(p2, "wcvp_distribution.csv"), sep="|",
                            header=TRUE, quote = "", fill=TRUE, na.strings = "", encoding = "UTF-8") 
```

We will also use some functions from the tidyverse.

```{r, message = F, warning = F}
library(tidyverse)
library(data.table)
```

The way that BarnebyLives matches spellings of names is very simple. 
We create three tables, one large table which contains all of the species in the WFO database, one moderately small table which contains all infraspecific taxa (varieties and subspecies), and a small table of all of the genera. 

On the way to completing this process, we will set up a file for a reasonable geographic extent. 
Since our instance focus on North America, we will only select taxa known to occur there using the Kew Distributions data set. 

```{r subset to area of interest, eval = F}
distributions <- distributions[distributions$continent == 'NORTHERN AMERICA'
                               & grepl('U.S.A.', distributions$region), 
                               c('plant_name_id')] # here we select for all of the U.S.A.
distributions <- unique(distributions)

names <- names[names$plant_name_id %in% distributions & names$taxon_rank != 'genus',
               c('plant_name_id', 'taxon_rank', 'family', 'genus', 'species', 'infraspecific_rank', 
                 'infraspecies', 'taxon_name', 'accepted_plant_name_id', 'parent_plant_name_id')]

names <-arrange(names, family, genus, species)
```

However, both the genus and specific epithet tables are abbreviated. We find the results from these tables based on only the leading first two or three characters. This gives rises to one of BarnebyLives(!) limitations, the assumption that the first two characters of the genus, and first three letters of the specific epithet are spelled correctly. 

```{r prepare materials for taxonomy look up tables, eval = F}

sppLKPtab <- names[, c('taxon_name', 'genus', 'species', 'infraspecific_rank',
                       'infraspecies', 'taxon_rank')]

sppLKPtab <- filter(sppLKPtab, taxon_rank == "Species") %>% 
  mutate(Grp = str_extract(species, '[a-z]{2}')) %>% 
  arrange(taxon_name)

infra_sppLKPtab <- filter(names, taxon_rank %in% c("Variety", "Subspecies")) %>% 
  mutate(Grp = str_extract(infraspecies, '[a-z]{2}')) %>%
  select('taxon_name', 'genus', 'species', 'infraspecific_rank',
                       'infraspecies', 'taxon_rank') %>% 
  arrange(taxon_name) 

genLKPtab <- filter(names, taxon_rank == 'Genus') %>% 
  select('taxon_name', 'genus', 'taxon_rank') %>% 
  arrange(genus) %>% 
  mutate(Grp = str_extract(genus, '[A-Z]{1}'))


families <- sort(unique(names$family))
families <- c(families, 'Hydrophyllaceae', 'Namaceae') #add on a few
families <- data.frame(Family = families)

write.csv(families, '/media/steppe/hdd/Barneby_Lives-dev/taxonomic_data/families_lookup_table.csv', row.names = F)
write.csv(sppLKPtab, '/media/steppe/hdd/Barneby_Lives-dev/taxonomic_data/species_lookup_table.csv', row.names = F)
write.csv(genLKPtab, '/media/steppe/hdd/Barneby_Lives-dev/taxonomic_data/genus_lookup_table.csv', row.names = F)
write.csv(infra_sppLKPtab, '/media/steppe/hdd/Barneby_Lives-dev/taxonomic_data/infra_species_lookup_table.csv', row.names = F)

rm(sppLKPtab, epiLKPtab, genLKPtab, cla, families)
```

```{r Prepare material for authorship lookup tables, eval = F}

abbrevs <- read.csv('/hdd/Barneby_Lives-dev/taxonomic_data_raw/ipni_author_abbreviations.csv')

trailed <- vector(mode = 'character', length = nrow(abbrevs))
trailed[grep('\\.$', abbrevs$author_abbrevation)] <- '.'
 # identify which abbreviations have a trailing period

abbrevs_spaced <- sub('\\.$', '', abbrevs$author_abbrevation) # remove the trailing periods
abbrevs_notrail <- sub('\\.(?!.*\\.)', ". ", 
    abbrevs_spaced, perl = T) # identify the last period in the name, and add a space after it

abbrevs <- paste0(abbrevs_notrail, trailed)

#usethis::use_data(abbrevs)
write.csv(abbrevs, row.names = F, '/hdd/Barneby_Lives-dev/taxonomic_data/abbrevs.csv')

rm(trailed, abbrevs_notrail, abbrevs_spaced, abbrevs)
```


## Geographic data

Many of these data sets are very large we are going to crop them to the area of analysis, and save the subsets. This package depends on **terra** and makes use of Virtual Raster Tiles to query parameters. 

```{r library, message = F}
library(sf)
library(terra)
```

You can readily define a simple custom extent for your study area. The coordinates are fine to grab and round from Google Maps or similar.
What is important is that the 5th set of coordinates is a repeat of the 1st set of coordinates. 

```{r define domain of data}

bound <- data.frame(
  y = c(30, 30, 50, 50, 30), 
  x = c(-85, -125, -125, -85, -85)
) %>% 
  st_as_sf(coords = c('x', 'y'), crs = 4326) %>% 
  st_bbox() %>% 
  st_as_sfc()

bb_vals <- c(-125, -85, 30, 50)
w_illi <- ext(bb_vals)
```

The data which we obtain our data sources from come as very large tiles. We will clip these down to the extent of our analysis. Essentially raster tiles should be no larger than 2 gigabytes. So we need to ensure that we create a tile grid which will allow us to save these data under those constraints. 

```{r create tile template grid}
tile_cells <- st_make_grid(
  bound,
  what = "polygons", square = T, flat_topped = F, crs = 4326,
  n = c(
  abs(round( (bb_vals[2] - bb_vals[1]) / 5, 0)), # rows
    abs(round( (bb_vals[3] - bb_vals[4]) / 5, 0))) #cols
) %>% 
  st_as_sf() %>% 
  rename(geometry = x) %>% 
  mutate(x = st_coordinates(st_centroid(.))[,1],
         y = st_coordinates(st_centroid(.))[,2], 
         .before = geometry, 
         across(c('x', 'y'), \(x) round(x, 1)),
         cellname = paste0('n', abs(y), 'w', abs(x))
         )

tile_cellsV <- vect(tile_cells)
```

This package comes with the 'mason' function, which will cut up the raster data sets, and save them in a new location. 

The source for our physical raster data is 'Geomorpho90m, empirical evaluation and accuracy assessment of global high-resolution geomorphometric layers'.
These data are available from here https://opentopography.s3.sdsc.edu/minio/dataspace/OTDS.012020.4326.1/raster/

```{r subset physical variables to domain, eval = F}

mason(dirin = '../geodata_raw/geomorphons/',
      dirout = '../geodata/geomorphons/',
      grid = tile_cellsV, fnames = 'cellname')

mason(dirin = '../geodata_raw/aspect/',
      dirout = '../geodata/aspect/',
      grid = tile_cellsV, fnames = 'cellname')

mason(dirin = '../geodata_raw/slope/',
      dirout = '../geodata/slope/',
      grid = tile_cellsV, fnames = 'cellname')
      
mason(dirin = '../geodata_raw/elevation/',
      dirout = '../geodata/elevation/',
      grid = tile_cellsV, fnames = 'cellname')

rm(mason)
```

We will simply crop the vector data to the limit of the study area. We utilize the U.S. Census Bureau's data for this. We can download the States data set from the U.S. Census Bureau's website. The other data sets can be loaded in from the tigris r package. 

```{r crop political boundary data to extent, eval = F}
counties <- st_read('../geodata_raw/USCounties/tl_2020_us_county.shp', quiet = T) %>% 
  st_transform(st_crs(tile_cells)) %>% 
  select(STATEFP, COUNTY = NAME)

states <- tigris::states() %>% 
  select(STATEFP, STATE = NAME, STUSPS) %>% 
  st_drop_geometry()

counties <- counties[st_intersects(counties, st_union(tile_cells)) %>% lengths > 0, ]
counties <- left_join(counties, states, by = "STATEFP") %>% 
  select(-STATEFP) %>% 
  arrange(STATE)

states <- counties %>%
  distinct(STUSPS) %>% 
  pull(STUSPS)

st_write(counties, dsn = file.path('../geodata/political', 'political.shp'))

rm(counties)
```

We will set up our places data set which we will use go gather driving directions from Google maps. 
```{r, eval = F}

bb <- pgirmess::bbox2sf(w = bb_vals[1], e = bb_vals[2],
                        s = bb_vals[3], n = bb_vals[4]) %>% 
  st_as_sf()

st <- tigris::states() %>% 
  st_transform(4326)
st <- st[st_intersects(st, bb) %>% lengths > 0, c('STATEFP', 'NAME', 'STUSPS') ]
places <- lapply(st$STUSPS, tigris::places)

places <- places %>% 
  bind_rows() %>% 
  filter(str_detect(NAMELSAD, 'city')) %>% 
  dplyr::select(STATEFP, NAME) %>% 
  sf::st_centroid() %>% 
  st_transform(4326) 

places <- places %>% 
  rename(CITY = NAME) %>% 
  left_join(., st %>% 
              st_drop_geometry(), by = 'STATEFP') %>% 
  st_transform(4326)

places <- places[st_intersects(places, bb) %>% lengths > 0, ]
places <- st_set_precision(places, precision=10^3)
st_write(places, 'places.shp', append = F) # this will drop location coordinates to 3 decimal places. 
places <- st_read('places.shp')

google_towns <- select(places, -STUSPS, -STATEFP, STATE = NAME) %>% 
  rowid_to_column("ID")

usethis::use_data(google_towns)

rm(places, google_towns)
```


We can access a Mountains dataset from EarthEnv which we can use for portions of our place names here https://www.earthenv.org/mountains.

```{r crop mountain names database to extent, eval = F}

mtns <- st_read('../geodata_raw/globalMountains/GMBA_Inventory_v2.0_standard_basic.shp', 
        quiet = T) %>% 
  st_make_valid() %>% 
  select(MapName) %>% 
  st_crop(., st_union(tile_cells))|>
  dplyr::rename(Mountains = MapName) |>
  dplyr::mutate(Mountains = stringr::str_remove(Mountains, '[(]nn[])]'))

st_write(mtns, dsn = file.path('../geodata/mountains', 'mountains.shp'), quiet = T)

rm(mtns)
```

Our place names are acquired from the United States Geological Surveys GNIS data set https://www.usgs.gov/tools/geographic-names-information-system-gnis .

```{r crop geographic place name database to extent, eval = F}

files <- paste0('../geodata_raw/GNIS/Text/DomesticNames_', states, '.txt')
cols <- c('feature_name', 'prim_lat_dec', 'prim_long_dec')

places <- lapply(files, read.csv, sep = "|") %>% 
  map(., ~ .x |> select(all_of(cols))) %>% 
  data.table::rbindlist() %>% 
  st_as_sf(coords = c('prim_long_dec', 'prim_lat_dec'), crs = 4269) %>% 
  st_transform(4326)

places <- places[st_intersects(places, st_union(tile_cells)) %>% lengths > 0, ]
places <- places %>% 
  mutate(ID = 1:nrow(.))

st_write(places, dsn = file.path('../geodata/places', 'places.shp'), quiet = T)

rm(places, cols, files)
```

To determine the land ownership of collections we utilize the Protected Areas Database-United States also from the USGS  https://www.usgs.gov/programs/gap-analysis-project/science/pad-us-data-download .

```{r crop padus to domain, eval = F}

padus <- st_read(dsn = '../geodata_raw/PADUS3/PAD_US3_0.gdb', 
                 layer = 'PADUS3_0Fee' %>% 
  filter(State_Nm %in% states) %>% 
  select(Mang_Name, Unit_Nm) %>% 
  st_cast('MULTIPOLYGON')

tile_cells <- st_transform(tile_cells, st_crs(padus))
padus <- padus[st_intersects(padus, st_union(tile_cells)) %>% lengths > 0, ]
padus <- st_transform(padus, crs = 4326)

st_write(padus, dsn = file.path('../geodata/pad', 'pad.shp'), quiet = T)

# replace really long state land board names with 'STATE SLB'

padus <- st_read('../../Barneby_Lives-dev/geodata/pad/pad.shp')

padus <- padus %>% 
  mutate(Unit_Nm = 
           str_replace(Unit_Nm, 
                               'The State of Utah School and Institutional Trust Lands Administration', 'Utah')
         )

st_write(padus, dsn = file.path('../geodata/pad', 'pad.shp'), quiet = T, append = F)

rm(padus)
```

The best geological data set are also, naturally, provided by the USGS. The geodatabase can be downloaded from here https://mrdata.usgs.gov/geology/state/ 

```{r geological map, eval = F}
geological <- st_read(
  '../geodata_raw/geologicalMap/USGS_StateGeologicMapCompilation_ver1.1.gdb', 
          layer = 'SGMC_Geology', quiet = T) %>% 
  select(GENERALIZED_LITH, UNIT_NAME) 

tile_cells <- st_transform(tile_cells, st_crs(geological))
geological <- geological[st_intersects(geological, st_union(tile_cells)) %>% lengths > 0, ]
geological <- st_crop(geological, st_union(tile_cells))
geological <- st_transform(geological, 4326)

st_write(geological, dsn = file.path('../geodata/geology', 'geology.shp'), quiet = T)

tile_cells <- st_transform(tile_cells, crs = 4326)
rm(geological)
```

Grazing allotments for the bureau of land management can be downloaded from https://gbp-blm-egis.hub.arcgis.com/datasets/blm-natl-grazing-allotment-polygons 

Grazing allotments for the United States Forest Service can be downloaded from ...
 
```{r allotments, eval = F}

blm <- st_read(
  '../geodata_raw/BLMAllotments/BLM_Natl_Grazing_Allotment_Polygons.shp') %>% 
  select(ALLOT_NAME)
usfs <- st_read(
  '../geodata_raw/USFSAllotment/S_USA.Allotment.shp', quiet = T
) %>% 
  select(ALLOT_NAME = ALLOTMENT_) 

allotments <- bind_rows(blm, usfs) %>% 
  st_make_valid()
tile_cells <- st_transform(tile_cells, st_crs(allotments))
allotments <- allotments[st_intersects(allotments, st_union(tile_cells)) %>% lengths > 0, ]
allotments <- st_crop(allotments, st_union(tile_cells))
allotments <- st_transform(allotments, 4326)

st_write(allotments, dsn = file.path('../geodata/allotments', 'allotments.shp'), quiet = T)

rm(blm, usfs, allotments)
```



```{r public land survey system only for land west of CO border, eval = F}

bound_nm <- data.frame(
  y = c(31, 31, 50, 50, 31), 
  x = c(-95, -125, -125, -95, -95)
) %>% 
  st_as_sf(coords = c('x', 'y'), crs = 4326) %>% 
  st_bbox() %>% 
  st_as_sfc()

township <- st_read(
  '../geodata_raw/nationalPLSS/ilmocplss.gdb', layer = 'PLSSTownship', quiet = T
) %>% 
  st_cast('POLYGON') %>% 
  select(TWNSHPLAB, PLSSID)

bound_nm <- st_transform(bound_nm, st_crs(township))
township <- township[st_intersects(township, bound_nm) %>% lengths > 0, ]
township <- st_drop_geometry(township)

section <- st_read(
  '../geodata_raw/nationalPLSS/ilmocplss.gdb', layer = 'PLSSFirstDivision', quiet = T
) %>% 
  st_cast('POLYGON') %>% 
  select(FRSTDIVLAB, PLSSID, FRSTDIVID)

section <- section[st_intersects(section, bound_nm) %>% lengths > 0, ]

trs <- inner_join(section, township, 'PLSSID') %>% 
  mutate(TRS = paste(TWNSHPLAB, FRSTDIVLAB)) %>% 
  select(TRS)

before <- object.size(trs)
trs <- st_simplify(trs)
after <- object.size(trs)
object.size(trs)

st_write(trs, 
         dsn = file.path('../geodata/plss', 'plss.shp'), quiet = T)

rm(section, township, trs, bound_nm, before, after)
```

```{r modify names of variables to be in proper formats, eval = F}

sf::st_read('../geodata/political/political.shp', quiet = T) %>% 
  rename(County = COUNTY, State = STATE) %>% 
  st_write(., '../geodata/political/political.shp', quiet = T, append = F)

sf::st_read('../geodata/allotments/allotments.shp', quiet = T) %>% 
  dplyr::mutate(Allotment = str_to_title(ALLOT_NAME)) %>% 
  dplyr::select(-ALLOT_NAME) %>% 
  st_write(., '../geodata/allotments/allotments.shp', quiet = T, append = F)

sf::st_read('../geodata/plss/plss.shp', quiet = T) %>% 
  rename(trs = TRS) %>% 
  st_write(., '../geodata/plss/plss.shp', quiet = T, append = F)

sf::st_read('../geodata/pad/pad.shp', quiet = T) %>% 
  sf::st_make_valid() %>% 
  dplyr::filter(sf::st_is_valid(.)) %>% 
  mutate(Unit_Nm = str_replace(Unit_Nm, "National Forest", "NF"),
         Unit_Nm = str_replace(Unit_Nm, "District Office", "DO"), 
         Unit_Nm = str_replace(Unit_Nm, "Field Office", "FO"), 
         Unit_Nm = str_replace(Unit_Nm, "National Park", "NP"),
         Unit_Nm = str_replace(Unit_Nm, 'National Grassland', 'NG'),
         Unit_Nm = str_replace(Unit_Nm, "National Wildlife Refuge|National Wildlife Range", "NWR"),
         Unit_Nm = str_replace(Unit_Nm, 'National Monument', 'NM'),
         Unit_Nm = str_replace(Unit_Nm, "National Recreation Area", "NRA"), 
         Unit_Nm = str_replace(Unit_Nm, "State Resource Management Area", "SRMA"), 
         Unit_Nm = str_replace(Unit_Nm, "State Trust Lands", "STL"), 
         Unit_Nm = str_replace(Unit_Nm, "Department", "Dept."),
         Unit_Nm = str_replace(Unit_Nm, "Recreation Area", "Rec. Area"),
         Unit_Nm = str_replace(Unit_Nm, 'Wildlife Habitat Management Area', "WHMA"), 
         Unit_Nm = str_replace(Unit_Nm, 'Wildlife Management Area', "WMA")) %>% 
  st_write(., '../geodata/pad/pad.shp', quiet = T, append = F)


sf::st_read('../geodata/places/places.shp', quiet = T) %>% 
  mutate(
    fetr_nm = str_remove(fetr_nm, ' Census Designated Place' ),
    fetr_nm = str_remove(fetr_nm, 'Township of '), 
    fetr_nm = str_remove(fetr_nm, ' \\(historical\\)'),
    fetr_nm = str_remove(fetr_nm, 'Village of '),
    fetr_nm = str_remove(fetr_nm, 'Town of '), 
    fetr_nm = str_remove(fetr_nm, 'City of ')
    ) %>% 
  filter(!str_detect(fetr_nm,
                     'Election Precinct| County|Ditch|Unorganized Territory|Canal|Lateral|Division|
                     River Division|Drain Number|Waste Pond|Commissioner District|Lake Superior|Lake Michigan')) %>% 
  mutate(
    fetr_nm = str_replace(fetr_nm, ' Canyon', ' cnyn.'),
    fetr_nm = str_replace(fetr_nm, ' Campground', ' cmpgrnd.'),
    fetr_nm = str_replace(fetr_nm, ' Creek', ' crk.'),
    fetr_nm = str_replace(fetr_nm, ' Fork', ' fk.'),
    fetr_nm = str_replace(fetr_nm, ' Lake', ' lk.'),
    fetr_nm = str_replace(fetr_nm, ' Meadows', ' mdws.'),
    fetr_nm = str_replace(fetr_nm, ' Meadow', ' mdw.'),
    fetr_nm = str_replace(fetr_nm, ' Mountains', ' mtns.'),
    fetr_nm = str_replace(fetr_nm, ' Mountain', ' mtn.'),
    fetr_nm = str_replace(fetr_nm, ' Number', ' #'),
    fetr_nm = str_replace(fetr_nm, ' Point', ' pt.'),
    fetr_nm = str_replace(fetr_nm, ' Reservoir', ' rsvr.'),
    fetr_nm = str_replace(fetr_nm, ' River ', ' rvr.')
  ) %>% 
  st_write(., '../geodata/places/places.shp', quiet = T, append = F)

```


# add files for templates

```{r, eval = F}
herbaria_info <- read.csv('/media/sagesteppe/ExternalHD/Barneby_Lives-dev/herbarium_data/index_herbariorum_info.csv')
usethis::use_data(herbaria_info)
```

```{r, eval = F}
collection_examples <- read.csv('/media/sagesteppe/ExternalHD/Barneby_Lives-dev/herbarium_data/shipping_examples.csv')
collection_examples[collection_examples==""] <- NA
usethis::use_data(collection_examples, overwrite = TRUE)
```


```{r, eval = F}
project_examples <- 
  read.csv('/media/sagesteppe/ExternalHD/Barneby_Lives-dev/herbarium_data/project.csv')
usethis::use_data(project_examples)
```

