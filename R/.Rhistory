# devtools::install_github('sagesteppe/BarnebyLives')
library(tidyverse)
library(BarnebyLives)
library(googlesheets4)
library(textclean)
rr <- read_sheet('1iOQBNeGqRJ3yhA-Sujas3xZ2Aw5rFkktUKv3N_e4o8M',
sheet = 'Processed California - 2024') |>
filter(Primary_Collector == 'Rosalind Rowe') |>
mutate(across(.cols = Associates:Vegetation, na_if(.x, 'NA')))
rr <- read_sheet('1iOQBNeGqRJ3yhA-Sujas3xZ2Aw5rFkktUKv3N_e4o8M',
sheet = 'Processed California - 2024') |>
filter(Primary_Collector == 'Rosalind Rowe') |>
mutate(across(.cols = Associates:Vegetation, ~ na_if(.x, 'NA')))
View(rr)
rr$Vegetation
rr <- format_database_import(rr, 'JEPS') |>
mutate(across(everything(), ~ as.character(.))) |>
mutate(across(everything(), ~ replace_na(., '')))  |>
mutate(
Coordinate_Uncertainty_In_Meters = '5',
Coordinate_Source = 'GPS',
)
write.csv(rr, '../results/RosalindRowe_SOS2024_Jepson.csv', row.names = F)
rr <- format_database_import(rr, 'JEPS') |>
mutate(across(everything(), ~ as.character(.))) |>
mutate(across(everything(), ~ replace_na(., '')))  |>
mutate(
Coordinate_Uncertainty_In_Meters = '5',
Coordinate_Source = 'GPS',
Label_Footer = 'Collected under the auspices of the Bureau of Land Management'
)
rr <- read_sheet('1iOQBNeGqRJ3yhA-Sujas3xZ2Aw5rFkktUKv3N_e4o8M',
sheet = 'Processed California - 2024') |>
filter(Primary_Collector == 'Rosalind Rowe') |>
mutate(across(.cols = Associates:Vegetation, ~ na_if(.x, 'NA')))
rr <- format_database_import(rr, 'JEPS') |>
mutate(across(everything(), ~ as.character(.))) |>
mutate(across(everything(), ~ replace_na(., '')))  |>
mutate(
Coordinate_Uncertainty_In_Meters = '5',
Coordinate_Source = 'GPS',
Label_Footer = 'Collected under the auspices of the Bureau of Land Management'
)
write.csv(rr, '../results/RosalindRowe_SOS2024_Jepson.csv', row.names = F)
View(rr)
devtools::install_github('sagesteppe/BarnebyLives')
library(tidyverse)
library(BarnebyLives)
library(googlesheets4)
processed <- read_sheet('1iOQBNeGqRJ3yhA-Sujas3xZ2Aw5rFkktUKv3N_e4o8M',
sheet = 'Processed COPL - 2024') %>%
mutate(UNIQUEID = paste0(Primary_Collector, Collection_number),
Coordinate_Uncertainty = '+/- 5m') %>%
# select(-Directions_BL) %>%
data.frame()
processed <- data.frame( apply(processed, 2, as.character) )
processed <- mutate(processed, Collection_number = as.numeric(Collection_number))
proc_split <- split(processed, processed$Primary_Collector)
fnames <- paste0('../results/collections-', gsub(' ', '_', names(proc_split)), '.csv')
for(i in 1:length(proc_split)) {
write.csv(proc_split[[i]], fnames[i], row.names = F)
}
dir.create('../HerbariumLabels/raw/Katie-raw')
p <- '/media/steppe/hdd/2024SOS_HerbariumSpecimens/HerbariumLabels/raw/Katie-raw'
purrr::walk(
.x = proc_split[['Katie Peel']]$Collection_number,
~ rmarkdown::render(
input = 'skeleton-Katie.Rmd',
output_file = file.path(p, glue::glue("{.x}.pdf")),
params = list(Collection_number = {.x})
)
)
dir.create('../HerbariumLabels/raw/Phoebe-raw')
p <- '/media/steppe/hdd/2024SOS_HerbariumSpecimens/HerbariumLabels/raw/Phoebe-raw'
purrr::walk(
.x = proc_split[['Phoebe Smurthwaite']]$Collection_number,
~ rmarkdown::render(
input = 'skeleton-Phoebe.Rmd',
output_file = file.path(p, glue::glue("{.x}.pdf")),
params = list(Collection_number = {.x})
)
)
library(tidyverse)
library(BarnebyLives)
library(googlesheets4)
processed <- read_sheet('1iOQBNeGqRJ3yhA-Sujas3xZ2Aw5rFkktUKv3N_e4o8M',
sheet = 'Processed COPL - 2024') %>%
mutate(UNIQUEID = paste0(Primary_Collector, Collection_number),
Coordinate_Uncertainty = '+/- 5m') %>%
# select(-Directions_BL) %>%
data.frame()
processed <- data.frame( apply(processed, 2, as.character) )
processed <- mutate(processed, Collection_number = as.numeric(Collection_number))
proc_split <- split(processed, processed$Primary_Collector)
fnames <- paste0('../results/collections-', gsub(' ', '_', names(proc_split)), '.csv')
for(i in 1:length(proc_split)) {
write.csv(proc_split[[i]], fnames[i], row.names = F)
}
dir.create('../HerbariumLabels/raw')
dir.create('../HerbariumLabels/raw/Phoebe-raw')
p <- '/media/steppe/hdd/2024SOS_HerbariumSpecimens/HerbariumLabels/raw/Phoebe-raw'
purrr::walk(
.x = proc_split[['Phoebe Smurthwaite']]$Collection_number,
~ rmarkdown::render(
input = 'skeleton-Phoebe.Rmd',
output_file = file.path(p, glue::glue("{.x}.pdf")),
params = list(Collection_number = {.x})
)
)
p2script <- paste0(.libPaths()[
grepl(paste0(version$major, '.', sub('\\..*', "", version$minor)),
.libPaths())], '/render_labels.sh')
dir.create('../HerbariumLabels/raw/Phoebe-raw')
library(BarnebyLives)
data(names_vec)
?file.remove
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = , bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, bound, cleanup){
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = , bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, bound, cleanup){
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = , bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, bound, cleanup){
if(missing(path)){path <- '.'}
if(missing(cleanup)){default = FALSE}
# create the extent which we are operating in
bb_vals <- c(min(bound$x), max(bound$x), min(bound$y), max(bound$y))
bound <- bound |>
sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>
sf::st_bbox() |>
sf::st_as_sfc()
# use tiles to create many smaller rasters, rather than one really big raster.
mt <- make_tiles(bound, bb_vals)
tile_cells <- mt$tile_cells
tile_cellsV <- mt$tile_cellsV
# decompress all of the archives so we can readily read them into R.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
unzip(zzzs)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
mason(dirin = file.path(path,  'geomorphons',
dirout = file.path(path, 'geodata', 'geomorphons'),
grid = tile_cellsV, fnames = 'cellname')
mason(dirin = file.path(path, 'aspect'),
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = , bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, bound, cleanup){
if(missing(path)){path <- '.'}
if(missing(cleanup)){default = FALSE}
# create the extent which we are operating in
bb_vals <- c(min(bound$x), max(bound$x), min(bound$y), max(bound$y))
bound <- bound |>
sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>
sf::st_bbox() |>
sf::st_as_sfc()
# use tiles to create many smaller rasters, rather than one really big raster.
mt <- make_tiles(bound, bb_vals)
tile_cells <- mt$tile_cells
tile_cellsV <- mt$tile_cellsV
# decompress all of the archives so we can readily read them into R.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
unzip(zzzs)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
mason(dirin = file.path(path,  'geomorphons'),
dirout = file.path(path, 'geodata', 'geomorphons'),
grid = tile_cellsV, fnames = 'cellname')
mason(dirin = file.path(path, 'aspect'),
dirout = file.path(path, 'geodata', 'aspect'),
grid = tile_cellsV, fnames = 'cellname')
mason(dirin = file.path(path, 'slope'),
dirout = file.path(path, 'geodata', 'slope'),
grid = tile_cellsV, fnames = 'cellname')
mason(dirin = file.path(path, 'elevation'),
dirout = file.path(path, 'geodata', 'elevation'),
grid = tile_cellsV, fnames = 'cellname')
## now combine all of the administrative data into a single vector file.
make_it_political(path)
# Set up Places data set, these are localities we can use google maps to get
# directions from https://www.youtube.com/watch?v=skZPzx-XlD0
places_and_spaces(bound)
#### process the GMBA data
process_gmba(path, tile_cells)
### crop geographic place name database to extent
process_gnis(path, tile_cells)
## crop PADUS to domain
process_padus(tile_cells)
## geological map
geological_map(tile_cells)
## Process grazing allotments
process_grazing_allot(tile_cells)
## Public land survey system
process_plss(tile_cells)
if(cleanup==TRUE){
# remove original zip files.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
file.remove(zzzs)
}
}
View(data_setup)
