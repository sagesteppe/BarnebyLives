lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
#  mason(dirin = file.path(path,  'geomorphons'),
#        dirout = file.path(path, 'geodata', 'geomorphons'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'aspect'),
#        dirout = file.path(path, 'geodata', 'aspect'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'slope'),
#        dirout = file.path(path, 'geodata', 'slope'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'elevation'),
#        dirout = file.path(path, 'geodata', 'elevation'),
#        grid = tile_cellsV, fnames = 'cellname')
## now combine all of the administrative data into a single vector file.
make_it_political(path, pathOut, tile_cells)
# Set up Places data set, these are localities we can use google maps to get
# directions from https://www.youtube.com/watch?v=skZPzx-XlD0
dir.create(file.path(pathOut, 'geodata', 'places'), showWarnings = FALSE)
# places_and_spaces(path, pathOut, bound)
#### process the GMBA data
process_gmba(path, pathOut, tile_cells)
### crop geographic place name database to extent
process_gnis(path, pathOut, bound, tile_cells)
## crop PADUS to domain
# process_padus(path, pathOut, tile_cells)
## geological map
geological_map(path, pathOut, tile_cells)
## Process grazing allotments
process_grazing_allot(path, pathOut, tile_cells)
## Public land survey system
process_plss(path, pathOut, tile_cells)
if(cleanup==TRUE){
# remove original zip files.
zzzs <- list.files(path = path, pattern = '*.[.]zip$')
file.remove(zzzs)
}
}
data_setup(bound = bound)
#' Set up the downloaded data for a BarnebyLives instance
#'
#' @description used within `data_setup`
process_gnis <- function(path, pathOut, bound, tile_cells){
bb <- sf::st_as_sf(bound)
st <- tigris::states(progress_bar = FALSE, year = 2022) %>%
sf::st_transform(4326)
st <- st[sf::st_intersects(st, bb) %>% lengths > 0, c('STATEFP', 'NAME', 'STUSPS') ]
### crop geographic place name database to extent
p2dat <- file.path(path, 'GNIS', 'Text')
files <- file.path(p2dat, list.files(p2dat))
patterns <- paste0('DomesticNames_', st$STUSPS, '.txt', collapse = '|')
files <- files[ grep(patterns, files)]
cols <- c('feature_name', 'prim_lat_dec', 'prim_long_dec')
places <- lapply(files, read.csv, sep = "|") %>%
purrr::map(., ~ .x |> dplyr::select(all_of(cols))) %>%
data.table::rbindlist() %>%
sf::st_as_sf(coords = c('prim_long_dec', 'prim_lat_dec'), crs = 4269) %>%
sf::st_transform(4326)
places <- places[sf::st_intersects(places, sf::st_union(tile_cells)) %>% lengths > 0, ]
places <- places %>%
dplyr::mutate(ID = 1:nrow(.))
sf::st_write(places, dsn = file.path(pathOut, 'geodata', 'places', 'place.shp'), quiet = T, append = F)
}
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = './geodata_raw', bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, pathOut, bound, cleanup){
if(missing(path)){path <- './geodata_raw'}
if(missing(pathOut)){pathOut <- '.'}
dir.create(file.path(pathOut, 'geodata'), showWarnings = FALSE)
if(missing(cleanup)){default = FALSE}
# create the extent which we are operating in
bb_vals <- c(min(bound$x), max(bound$x), min(bound$y), max(bound$y))
bound <- bound |>
sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>
sf::st_bbox() |>
sf::st_as_sfc()
# use tiles to create many smaller rasters, rather than one really big raster.
mt <- make_tiles(bound, bb_vals)
tile_cells <- mt$tile_cells
tile_cellsV <- mt$tile_cellsV
# decompress all of the archives so we can readily read them into R.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
#  out_dirs <- gsub('[.]zip', '' , zzzs)
#  lapply(out_dirs, dir.create)
#  return(zzzs)
lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
#  mason(dirin = file.path(path,  'geomorphons'),
#        dirout = file.path(path, 'geodata', 'geomorphons'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'aspect'),
#        dirout = file.path(path, 'geodata', 'aspect'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'slope'),
#        dirout = file.path(path, 'geodata', 'slope'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'elevation'),
#        dirout = file.path(path, 'geodata', 'elevation'),
#        grid = tile_cellsV, fnames = 'cellname')
## now combine all of the administrative data into a single vector file.
make_it_political(path, pathOut, tile_cells)
# Set up Places data set, these are localities we can use google maps to get
# directions from https://www.youtube.com/watch?v=skZPzx-XlD0
dir.create(file.path(pathOut, 'geodata', 'places'), showWarnings = FALSE)
# places_and_spaces(path, pathOut, bound)
#### process the GMBA data
process_gmba(path, pathOut, tile_cells)
### crop geographic place name database to extent
process_gnis(path, pathOut, bound, tile_cells)
## crop PADUS to domain
# process_padus(path, pathOut, tile_cells)
## geological map
geological_map(path, pathOut, tile_cells)
## Process grazing allotments
process_grazing_allot(path, pathOut, tile_cells)
## Public land survey system
process_plss(path, pathOut, tile_cells)
if(cleanup==TRUE){
# remove original zip files.
zzzs <- list.files(path = path, pattern = '*.[.]zip$')
file.remove(zzzs)
}
}
setwd('/home/sagesteppe/Documents/BL_sandbox')
data_setup(bound = bound)
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = './geodata_raw', bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, pathOut, bound, cleanup){
if(missing(path)){path <- './geodata_raw'}
if(missing(pathOut)){pathOut <- '.'}
dir.create(file.path(pathOut, 'geodata'), showWarnings = FALSE)
if(missing(cleanup)){default = FALSE}
# create the extent which we are operating in
bb_vals <- c(min(bound$x), max(bound$x), min(bound$y), max(bound$y))
bound <- bound |>
sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>
sf::st_bbox() |>
sf::st_as_sfc()
# use tiles to create many smaller rasters, rather than one really big raster.
mt <- make_tiles(bound, bb_vals)
tile_cells <- mt$tile_cells
tile_cellsV <- mt$tile_cellsV
# decompress all of the archives so we can readily read them into R.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
#  out_dirs <- gsub('[.]zip', '' , zzzs)
#  lapply(out_dirs, dir.create)
#  return(zzzs)
lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
#  mason(dirin = file.path(path,  'geomorphons'),
#        dirout = file.path(path, 'geodata', 'geomorphons'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'aspect'),
#        dirout = file.path(path, 'geodata', 'aspect'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'slope'),
#        dirout = file.path(path, 'geodata', 'slope'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'elevation'),
#        dirout = file.path(path, 'geodata', 'elevation'),
#        grid = tile_cellsV, fnames = 'cellname')
## now combine all of the administrative data into a single vector file.
make_it_political(path, pathOut, tile_cells)
# Set up Places data set, these are localities we can use google maps to get
# directions from https://www.youtube.com/watch?v=skZPzx-XlD0
dir.create(file.path(pathOut, 'geodata', 'places'), showWarnings = FALSE)
# places_and_spaces(path, pathOut, bound)
#### process the GMBA data
process_gmba(path, pathOut, tile_cells)
### crop geographic place name database to extent
process_gnis(path, pathOut, bound, tile_cells)
## crop PADUS to domain
# process_padus(path, pathOut, tile_cells)
## geological map
#  geological_map(path, pathOut, tile_cells)
## Process grazing allotments
process_grazing_allot(path, pathOut, tile_cells)
## Public land survey system
process_plss(path, pathOut, tile_cells)
if(cleanup==TRUE){
# remove original zip files.
zzzs <- list.files(path = path, pattern = '*.[.]zip$')
file.remove(zzzs)
}
}
data_setup(bound = bound)
#' Set up the downloaded data for a BarnebyLives instance
#'
#' @description used within `data_setup`
process_grazing_allot <- function(path, pathOut, tile_cells){
p <- file.path(path, 'BLMAllotments', 'BLM_Natl_Grazing_Allotment_Polygons.shp')
p1 <- file.path(path, 'USFSAllotments', 'S_USA.Allotment.shp')
blm <- sf::st_read(p, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME)
usfs <- sf::st_read(p1, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME = ALLOTMENT_)
allotments <- dplyr::bind_rows(blm, usfs) %>%
sf::st_make_valid()
tile_cells <- sf::st_transform(tile_cells, sf::st_crs(allotments))
allotments <- allotments[sf::st_intersects(allotments, sf::st_union(tile_cells)) %>% lengths > 0, ]
allotments <- sf::st_crop(allotments, sf::st_union(tile_cells))
allotments <- sf::st_transform(allotments, 4326) %>%
dplyr::mutate(Allotment = stringr::str_to_title(ALLOT_NAME)) %>%
dplyr::select(-ALLOT_NAME) %>%
sf::st_write(
allotments,  quiet = T,
dsn = file.path(pathOut, 'geodata', 'allotments', 'allotments.shp')
)
}
#' Set up the downloaded data for a BarnebyLives instance
#'
#' @description used within `data_setup`
process_grazing_allot <- function(path, pathOut, tile_cells){
p <- file.path(path, 'BLMAllotments', 'BLM_Natl_Grazing_Allotment_Polygons.shp')
p1 <- file.path(path, 'USFSAllotments', 'S_USA.Allotment.shp')
blm <- sf::st_read(p, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME)
usfs <- sf::st_read(p1, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME = ALLOTMENT_)
allotments <- dplyr::bind_rows(blm, usfs) %>%
sf::st_make_valid()
tile_cells <- sf::st_transform(tile_cells, sf::st_crs(allotments))
allotments <- allotments[sf::st_intersects(allotments, sf::st_union(tile_cells)) %>% lengths > 0, ]
allotments <- sf::st_crop(allotments, sf::st_union(tile_cells))
allotments <- sf::st_transform(allotments, 4326) %>%
dplyr::mutate(Allotment = stringr::str_to_title(ALLOT_NAME)) %>%
dplyr::select(-ALLOT_NAME) %>%
dir.create(file.path(pathOut, 'geodata', 'allotments'), showWarning = FALSE)
sf::st_write(
allotments,  quiet = TRUE, append = FALSE,
dsn = file.path(pathOut, 'geodata', 'allotments', 'allotments.shp')
)
}
data_setup(bound = bound)
?dir.create
#' Set up the downloaded data for a BarnebyLives instance
#'
#' @description used within `data_setup`
process_grazing_allot <- function(path, pathOut, tile_cells){
p <- file.path(path, 'BLMAllotments', 'BLM_Natl_Grazing_Allotment_Polygons.shp')
p1 <- file.path(path, 'USFSAllotments', 'S_USA.Allotment.shp')
blm <- sf::st_read(p, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME)
usfs <- sf::st_read(p1, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME = ALLOTMENT_)
allotments <- dplyr::bind_rows(blm, usfs) %>%
sf::st_make_valid()
tile_cells <- sf::st_transform(tile_cells, sf::st_crs(allotments))
allotments <- allotments[sf::st_intersects(allotments, sf::st_union(tile_cells)) %>% lengths > 0, ]
allotments <- sf::st_crop(allotments, sf::st_union(tile_cells))
allotments <- sf::st_transform(allotments, 4326) %>%
dplyr::mutate(Allotment = stringr::str_to_title(ALLOT_NAME)) %>%
dplyr::select(-ALLOT_NAME) %>%
dir.create(file.path(pathOut, 'geodata', 'allotments'), showWarnings = FALSE)
sf::st_write(
allotments,  quiet = TRUE, append = FALSE,
dsn = file.path(pathOut, 'geodata', 'allotments', 'allotments.shp')
)
}
?dir.create
data_setup(bound = bound)
dir.create(file.path(pathOut, 'geodata', 'allotments'), showWarnings = FALSE)
#' Set up the downloaded data for a BarnebyLives instance
#'
#' @description used within `data_setup`
process_grazing_allot <- function(path, pathOut, tile_cells){
p <- file.path(path, 'BLMAllotments', 'BLM_Natl_Grazing_Allotment_Polygons.shp')
p1 <- file.path(path, 'USFSAllotments', 'S_USA.Allotment.shp')
blm <- sf::st_read(p, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME)
usfs <- sf::st_read(p1, quiet = TRUE) %>%
dplyr::select(ALLOT_NAME = ALLOTMENT_)
allotments <- dplyr::bind_rows(blm, usfs) %>%
sf::st_make_valid()
tile_cells <- sf::st_transform(tile_cells, sf::st_crs(allotments))
allotments <- allotments[sf::st_intersects(allotments, sf::st_union(tile_cells)) %>% lengths > 0, ]
allotments <- sf::st_crop(allotments, sf::st_union(tile_cells))
allotments <- sf::st_transform(allotments, 4326) %>%
dplyr::mutate(Allotment = stringr::str_to_title(ALLOT_NAME)) %>%
dplyr::select(-ALLOT_NAME)
dir.create(file.path(pathOut, 'geodata', 'allotments'), showWarnings = FALSE)
sf::st_write(
allotments,  quiet = TRUE, append = FALSE,
dsn = file.path(pathOut, 'geodata', 'allotments', 'allotments.shp')
)
}
data_setup(bound = bound)
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = './geodata_raw', bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, pathOut, bound, cleanup){
if(missing(path)){path <- './geodata_raw'}
if(missing(pathOut)){pathOut <- '.'}
dir.create(file.path(pathOut, 'geodata'), showWarnings = FALSE)
if(missing(cleanup)){default = FALSE}
# create the extent which we are operating in
bb_vals <- c(min(bound$x), max(bound$x), min(bound$y), max(bound$y))
bound <- bound |>
sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>
sf::st_bbox() |>
sf::st_as_sfc()
# use tiles to create many smaller rasters, rather than one really big raster.
mt <- make_tiles(bound, bb_vals)
tile_cells <- mt$tile_cells
tile_cellsV <- mt$tile_cellsV
# decompress all of the archives so we can readily read them into R.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
#  out_dirs <- gsub('[.]zip', '' , zzzs)
#  lapply(out_dirs, dir.create)
#  return(zzzs)
lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
#  mason(dirin = file.path(path,  'geomorphons'),
#        dirout = file.path(path, 'geodata', 'geomorphons'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'aspect'),
#        dirout = file.path(path, 'geodata', 'aspect'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'slope'),
#        dirout = file.path(path, 'geodata', 'slope'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'elevation'),
#        dirout = file.path(path, 'geodata', 'elevation'),
#        grid = tile_cellsV, fnames = 'cellname')
## now combine all of the administrative data into a single vector file.
make_it_political(path, pathOut, tile_cells)
# Set up Places data set, these are localities we can use google maps to get
# directions from https://www.youtube.com/watch?v=skZPzx-XlD0
dir.create(file.path(pathOut, 'geodata', 'places'), showWarnings = FALSE)
# places_and_spaces(path, pathOut, bound)
#### process the GMBA data
process_gmba(path, pathOut, tile_cells)
### crop geographic place name database to extent
process_gnis(path, pathOut, bound, tile_cells)
## crop PADUS to domain
# process_padus(path, pathOut, tile_cells)
## geological map
#  geological_map(path, pathOut, tile_cells)
## Process grazing allotments
process_grazing_allot(path, pathOut, tile_cells)
## Public land survey system
#  process_plss(path, pathOut, tile_cells)
if(cleanup==TRUE){
# remove original zip files.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
}
}
#' Either mode of running the function will delete the uncompressed zip files generated during the process.
#' @examples \donttest{
#' bound <- data.frame(
#' y = c(30, 30, 50, 50, 30),
#' x = c(-85, -125, -125, -85, -85)
#' )
#'
#' data_setup(path = './geodata_raw', bound = bound, cleanup = FALSE)
#' }
#' @export
data_setup <- function(path, pathOut, bound, cleanup){
if(missing(path)){path <- './geodata_raw'}
if(missing(pathOut)){pathOut <- '.'}
dir.create(file.path(pathOut, 'geodata'), showWarnings = FALSE)
if(missing(cleanup)){default = FALSE}
# create the extent which we are operating in
bb_vals <- c(min(bound$x), max(bound$x), min(bound$y), max(bound$y))
bound <- bound |>
sf::st_as_sf(coords = c('x', 'y'), crs = 4326) |>
sf::st_bbox() |>
sf::st_as_sfc()
# use tiles to create many smaller rasters, rather than one really big raster.
mt <- make_tiles(bound, bb_vals)
tile_cells <- mt$tile_cells
tile_cellsV <- mt$tile_cellsV
# decompress all of the archives so we can readily read them into R.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
#  out_dirs <- gsub('[.]zip', '' , zzzs)
#  lapply(out_dirs, dir.create)
#  return(zzzs)
lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
##### now crop the data to the extents of analysis. ####
# start by making tiles of the landform variables #
#  mason(dirin = file.path(path,  'geomorphons'),
#        dirout = file.path(path, 'geodata', 'geomorphons'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'aspect'),
#        dirout = file.path(path, 'geodata', 'aspect'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'slope'),
#        dirout = file.path(path, 'geodata', 'slope'),
#        grid = tile_cellsV, fnames = 'cellname')
#  mason(dirin = file.path(path, 'elevation'),
#        dirout = file.path(path, 'geodata', 'elevation'),
#        grid = tile_cellsV, fnames = 'cellname')
## now combine all of the administrative data into a single vector file.
make_it_political(path, pathOut, tile_cells)
# Set up Places data set, these are localities we can use google maps to get
# directions from https://www.youtube.com/watch?v=skZPzx-XlD0
dir.create(file.path(pathOut, 'geodata', 'places'), showWarnings = FALSE)
# places_and_spaces(path, pathOut, bound)
#### process the GMBA data
process_gmba(path, pathOut, tile_cells)
### crop geographic place name database to extent
process_gnis(path, pathOut, bound, tile_cells)
## crop PADUS to domain
# process_padus(path, pathOut, tile_cells)
## geological map
#  geological_map(path, pathOut, tile_cells)
## Process grazing allotments
process_grazing_allot(path, pathOut, tile_cells)
## Public land survey system
#  process_plss(path, pathOut, tile_cells)
if(cleanup==TRUE){
# remove original zip files.
zzzs <- file.path(path, list.files(path = path, pattern = '*.[.]zip$'))
lapply(zzzs, FUN = function(x){
out <- gsub('[.]zip', '' , x)
unzip(x, exdir = out)}
)
}
}
list.dirs('geodata_raw')
list.dirs('geodata_raw', recursive = FALSE)
?dir.remove()
dirs <- list.dirs('geodata_raw', recursive = FALSE)
lapply(dirs, unlink, recursive = TRUE)
?dirs
?unlink
gc()
bucket_exists("s3://prod-is-usgs-sb-prod-content/")
aws.s3::bucket_exists("s3://prod-is-usgs-sb-prod-content/")
aws.s3::bucket_exists("s3://prod-is-usgs-sb-prod-content/", region = 'us-west-2')
aws.s3::bucket_exists(
bucket = "s3://prod-is-usgs-sb-prod-content/",
region = 'us-west-2')
aws.s3::bucket_exists(
bucket = "s3://prod-is/",
region = 'us-west-2'
)
?aws.s3
aws.s3::bucket_exists(
bucket = "s3://prod-is-usgs-sb-prod-content/",
region = 'us-west-2'
)
prod-is-usgs-sb-prod-content.s3.amazonaws.com
install.packages('sbtools')
library(sbtools)
library(sbtools)
install.packages('sodium')
install.packages('sodium')
install.packages('sbtools')
library(sbtools)
query_sb_text('Antarctica', limit=1)
query_sb_text('PADUS', limit=1)
query_sb_doi('PADUS', limit=1)
query_sb_doi('https://doi.org/10.5066/P96WBCHS', limit=1)
query_sb_doi('10.5066/P96WBCHS', limit=1)
children <- item_list_children('5474ec49e4b04d7459a7eab2')
children
children <- item_list_children('10.5066/P96WBCHS')
query_sb_doi('10.5066/P96WBCHS', limit=1)
query_sb_doi('10.5066/P96WBCHS', limit=1)
children <- item_list_children('5474ec49e4b04d7459a7eab2')
children
sapply(children, function(child) child$title)
query_sb_doi('10.5066/P96WBCHS', limit=1)
item_get(65294599d34e44db0e2ed7cf)
item_get('65294599d34e44db0e2ed7cf')
