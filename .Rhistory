}
dirs <- get_google_directions(x = df, api_key = SoS_gkey)
directions_overview(dirs)
#' @examples
#' library(sf)
#' SoS_gkey = Sys.getenv("Sos_gkey") # import the key saved in the system environment
#' df <- data.frame(latitude_dd = c(39.2558, 41.1234),
#'      longitude_dd = c(-117.6827, -119.1234),
#'      Site_name = LETTERS[1:2]) |>
#'      sf::st_as_sf(coords = c(x = 'longitude_dd', y = 'latitude_dd'), crs = 4326, remove = FALSE)
#' dirs <- get_google_directions(x = df, apikey = SoS_gkey)
#' head(dirs)
#' @export
get_google_directions <- function(x, api_key){
# extract the coordinates to serve as the DESTINATION
coords <- lapply(x, '[', c( 'latitude_dd', 'longitude_dd'))
coords <- lapply(coords, sf::st_drop_geometry)
coords <- do.call(rbind, coords)
coords <- paste(coords$latitude_dd, coords$longitude_dd,  sep = ", ")
# identify the ORIGIN
data('places', envir=environment())
x <- dplyr::bind_rows(x)
origin <- places[ sf::st_nearest_feature(x, places), c('CITY', 'NAME')] |>
sf::st_drop_geometry()
origin <- paste0(origin$CITY, ', ', origin$NAME)
rm(places)
# run query
directions <- Map(f = googleway::google_directions, origin = origin,
destination = coords, key = api_key,
mode = "driving", simplify = TRUE)
# rename output to reflect either ORIGIN-SITE_NAME, ORIGIN_DEST_COORDS
#names(directions) <- paste0(x$Site_name, '-', origin)
return(origin)
}
dirs <- get_google_directions(x = df, api_key = SoS_gkey)
dirs
#' @examples
#' library(sf)
#' SoS_gkey = Sys.getenv("Sos_gkey") # import the key saved in the system environment
#' df <- data.frame(latitude_dd = c(39.2558, 41.1234),
#'      longitude_dd = c(-117.6827, -119.1234),
#'      Site_name = LETTERS[1:2]) |>
#'      sf::st_as_sf(coords = c(x = 'longitude_dd', y = 'latitude_dd'), crs = 4326, remove = FALSE)
#' dirs <- get_google_directions(x = df, apikey = SoS_gkey)
#' head(dirs)
#' @export
get_google_directions <- function(x, api_key){
# extract the coordinates to serve as the DESTINATION
coords <- lapply(x, '[', c( 'latitude_dd', 'longitude_dd'))
coords <- lapply(coords, sf::st_drop_geometry)
coords <- do.call(rbind, coords)
coords <- paste(coords$latitude_dd, coords$longitude_dd,  sep = ", ")
# identify the ORIGIN
data('places', envir=environment())
x <- dplyr::bind_rows(x)
origin <- places[ sf::st_nearest_feature(x, places), c('CITY', 'NAME')] |>
sf::st_drop_geometry()
origin <- paste0(origin$CITY, ', ', origin$NAME)
rm(places)
# run query
directions <- Map(f = googleway::google_directions, origin = origin,
destination = coords, key = api_key,
mode = "driving", simplify = TRUE)
# rename output to reflect either ORIGIN-SITE_NAME, ORIGIN_DEST_COORDS
# names(directions) <-
u <- paste0(x$Site_name, '-', origin)
return(u)
}
dirs <- get_google_directions(x = df, api_key = SoS_gkey)
directions_overview(dirs)
dirs <- get_google_directions(x = df, api_key = SoS_gkey)
dirs
roxygen2::roxygenise()
roxygen2::roxygenise()
rm(list = c("get_google_directions"))
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
#' @returns messages to consoles indicating search terms, and there status if failed to be found. This desirable because 'powo_searcher' squashes these errors.
#' @examples  names_vec <- taxize::names_list(rank = 'species', size = 10)
#' # 10 random species from taxize, usually 1 or 2 species are not found in Plants of the world online
#' library(dplyr)
#' pow_results <- lapply(names_vec, powo_searcher) |>
#'   dplyr::bind_rows()
#' pow_results[,1:5]
#' # if there is not a family which is 'NOT FOUND', reshuffle the random species from taxize.
#' notFound(pow_results) # little message.
#' @export
notFound <- function(x){
library(crayon)
row_no <- unlist( apply(FUN = grep, X = x, MARGIN = 2, pattern = 'NOT FOUND') )
rows <- unique(row_no) # these rows had complications...
not_found <- x[rows, 'query'] # these records were not found.
recs <- cat(
'The record: ' %+%
crayon::blue$underline$bold(not_found) %+%
crayon::blue$bold(' (row ') %+%
crayon::blue$underline$bold(rows) %+% ') did not have all data retrieved.\n')
message(recs)
}
roxygen2::roxygenise()
roxygen2::roxygenise()
#' library(dplyr)
#' library(crayon)
#' names_vec <- taxize::names_list(rank = 'species', size = 10)
#' # 10 random species from taxize, usually 1 or 2 species are not found in Plants of the world online
#' pow_results <- lapply(names_vec, powo_searcher) |>
#'   dplyr::bind_rows()
#' # pow_results[,1:5]
#' # if there is not a family which is 'NOT FOUND', reshuffle the random species from taxize.
#' notFound(pow_results) # little message.
#' @export
notFound <- function(x){
require(crayon)
row_no <- unlist( apply(FUN = grep, X = x, MARGIN = 2, pattern = 'NOT FOUND') )
rows <- unique(row_no) # these rows had complications...
not_found <- x[rows, 'query'] # these records were not found.
recs <- cat(
'The record: ' %+%
crayon::blue$underline$bold(not_found) %+%
crayon::blue$bold(' (row ') %+%
crayon::blue$underline$bold(rows) %+% ') did not have all data retrieved.\n')
message(recs)
}
roxygen2::roxygenise()
rm(list = c("notFound"))
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
recs <- cat(
'The record: ' crayon::%+%
message(recs)
roxygen2::roxygenise()
requireNamespace(crayon)
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
places
load('data/places.rda')
rm(places)
load('data/places.rda')
roxygen2::roxygenise()
places
places$NAME
places
places
roxygen2::roxygenise()
#' library(dplyr)
#' library(crayon)
#' names_vec <- taxize::names_list(rank = 'species', size = 10)
#' # 10 random species from taxize, usually 1 or 2 species are not found in Plants of the world online
#' pow_results <- lapply(names_vec, powo_searcher) |>
#'   dplyr::bind_rows()
#' # pow_results[,1:5]
#' # if there is not a family which is 'NOT FOUND', reshuffle the random species from taxize.
#' notFound(pow_results) # little message.
#' @export
notFound <- function(x){
row_no <- unlist( apply(FUN = grep, X = x, MARGIN = 2, pattern = 'NOT FOUND') )
rows <- unique(row_no) # these rows had complications...
not_found <- x[rows, 'query'] # these records were not found.
recs <- cat(
'The record: ',
not_found,
' (row ',
rows, ') did not have all data retrieved.\n')
message(recs)
}
install.packages('kewr')
devtools::install_github('barnabywalker/kewr')
install.packages('googleway')
abbrevs <- read.csv('/hdd/Barneby_Lives-dev/taxonomic_data_raw/ipni_author_abbreviations.csv')
trailed <- vector(mode = 'character', length = nrow(abbrevs))
trailed[grep('\\.$', abbrevs$author_abbrevation)] <- '.'
abbrevs_spaced <- sub('\\.$', '', abbrevs$author_abbrevation) # remove the trailing periods
abbrevs_notrail <- sub('\\.(?!.*\\.)', ". ",
abbrevs_spaced, perl = T) # identify the last period in the name, and add a space after it
abbrevs <- paste0(abbrevs_notrail, trailed)
saveRDS(abbrevs, '/hdd/BarnebyLives/data/abbrevs.rda')
rm(trailed, abbrevs_notrail, abbrevs_spaced)
abbrevs <- read.csv('/hdd/Barneby_Lives-dev/taxonomic_data_raw/ipni_author_abbreviations.csv')
trailed <- vector(mode = 'character', length = nrow(abbrevs))
trailed[grep('\\.$', abbrevs$author_abbrevation)] <- '.'
# identify which abbreviations have a trailing period
abbrevs_spaced <- sub('\\.$', '', abbrevs$author_abbrevation) # remove the trailing periods
abbrevs_notrail <- sub('\\.(?!.*\\.)', ". ",
abbrevs_spaced, perl = T) # identify the last period in the name, and add a space after it
abbrevs <- paste0(abbrevs_notrail, trailed)
write.csv(abbrevs, row.names = F, '/hdd/BarnebyLives/data/abbrevs.csv')
load("/hdd/BarnebyLives/data/epiLKPtab.rda")
abbrevs <- read.csv('/hdd/Barneby_Lives-dev/taxonomic_data_raw/ipni_author_abbreviations.csv')
trailed <- vector(mode = 'character', length = nrow(abbrevs))
trailed[grep('\\.$', abbrevs$author_abbrevation)] <- '.'
abbrevs_spaced <- sub('\\.$', '', abbrevs$author_abbrevation) # remove the trailing periods
abbrevs_notrail <- sub('\\.(?!.*\\.)', ". ",
abbrevs_spaced, perl = T) # identify the last period in the name, and add a space after it
abbrevs <- paste0(abbrevs_notrail, trailed)
save(abbrevs, '/hdd/BarnebyLives/data/abbrevs.rda')
save(abbrevs, '/hdd/BarnebyLives/data/abbrevs')
saveRDS(abbrevs, '/hdd/BarnebyLives/data/abbrevs')
sppLKPtab <- cla[, c('scientificName', 'specificEpithet', 'infraspecificEpithet', 'verbatimTaxonRank')]
cla <- data.table::fread('/hdd/Barneby_Lives-dev/taxonomic_data_raw/WFO_Backbone/classification.txt', na.strings = "")
# install.packages('data.table') # this is in depends, but maybe you don't our package yet...
sppLKPtab <- cla[, c('scientificName', 'specificEpithet', 'infraspecificEpithet', 'verbatimTaxonRank')]
sppLKPtab <- filter(sppLKPtab, is.na(verbatimTaxonRank) | verbatimTaxonRank != "f.")
genLKPtab <- unique(cla, by = 'genus')[taxonomicStatus == 'Accepted','genus'] %>%
arrange(genus) %>%
mutate(Grp = str_extract(genus, '[A-Z][a-z]{1}'))%>%
rename(strings = genus)
library(BarnebyLives)
library(tidyverse)
genLKPtab <- unique(cla, by = 'genus')[taxonomicStatus == 'Accepted','genus'] %>%
arrange(genus) %>%
mutate(Grp = str_extract(genus, '[A-Z][a-z]{1}'))%>%
rename(strings = genus)
epiLKPtab <- unique(cla, by = 'specificEpithet')[
taxonomicStatus == 'Accepted','specificEpithet'] %>%
arrange(specificEpithet) %>%
mutate(Grp = str_extract(specificEpithet, '[a-z]{3}')) %>%
rename(strings = specificEpithet)
saveRDS(sppLKPtab, '/hdd/BarnebyLives/data/sppLKPtab')
saveRDS(epiLKPtab, '/hdd/BarnebyLives/data/epiLKPtab')
saveRDS(genLKPtab, '/hdd/BarnebyLives/data/genLKPtab')
?saveRDS
load("/hdd/BarnebyLives/data/sppLKPtab.rda")
load("/hdd/BarnebyLives/data/sppLKPtab.rda")
bb <- pgirmess::bbox2sf(w = bb_vals[1], e = bb_vals[2],
s = bb_vals[3], n = bb_vals[4]) %>%
st_as_sf()
cla <- data.table::fread('/hdd/Barneby_Lives-dev/taxonomic_data_raw/WFO_Backbone/classification.txt', na.strings = "")
# install.packages('data.table') # this is in depends, but maybe you don't our package yet...
sppLKPtab <- cla[, c('scientificName', 'specificEpithet', 'infraspecificEpithet', 'verbatimTaxonRank')]
sppLKPtab <- filter(sppLKPtab, is.na(verbatimTaxonRank) | verbatimTaxonRank != "f.")
genLKPtab <- unique(cla, by = 'genus')[taxonomicStatus == 'Accepted','genus'] %>%
arrange(genus) %>%
mutate(Grp = str_extract(genus, '[A-Z][a-z]{1}'))%>%
rename(strings = genus)
epiLKPtab <- unique(cla, by = 'specificEpithet')[
taxonomicStatus == 'Accepted','specificEpithet'] %>%
arrange(specificEpithet) %>%
mutate(Grp = str_extract(specificEpithet, '[a-z]{3}')) %>%
rename(strings = specificEpithet)
usethis::use_data(sppLKPtab)
usethis::use_data(epiLKPtab)
usethis::use_data(genLKPtab)
abbrevs <- read.csv('/hdd/Barneby_Lives-dev/taxonomic_data_raw/ipni_author_abbreviations.csv')
trailed <- vector(mode = 'character', length = nrow(abbrevs))
trailed[grep('\\.$', abbrevs$author_abbrevation)] <- '.'
# identify which abbreviations have a trailing period
abbrevs_spaced <- sub('\\.$', '', abbrevs$author_abbrevation) # remove the trailing periods
abbrevs_notrail <- sub('\\.(?!.*\\.)', ". ",
abbrevs_spaced, perl = T) # identify the last period in the name, and add a space after it
abbrevs <- paste0(abbrevs_notrail, trailed)
usethis::use_data(abbrevs)
#write.csv(abbrevs, row.names = F, '/hdd/BarnebyLives/data/abbrevs.csv')
rm(trailed, abbrevs_notrail, abbrevs_spaced)
rm(sppLKPtab, epiLKPtab. genLKPtab)
rm(sppLKPtab, epiLKPtab, genLKPtab)
rm(trailed, abbrevs_notrail, abbrevs_spaced, abbrevs)
rm(sppLKPtab, epiLKPtab, genLKPtab, cla)
library(sf)
library(terra)
bound <- data.frame(
y = c(30, 30, 50, 50, 30),
x = c(-85, -125, -125, -85, -85)
) %>%
st_as_sf(coords = c('x', 'y'), crs = 4326) %>%
st_bbox() %>%
st_as_sfc()
bb_vals <- c(-125, -85, 30, 50)
w_illi <- ext(bb_vals)
bb <- pgirmess::bbox2sf(w = bb_vals[1], e = bb_vals[2],
s = bb_vals[3], n = bb_vals[4]) %>%
st_as_sf()
install.packages('pgirmess')
bb <- pgirmess::bbox2sf(w = bb_vals[1], e = bb_vals[2],
s = bb_vals[3], n = bb_vals[4]) %>%
st_as_sf()
st <- tigris::states() %>%
st_transform(4326)
st <- st[st_intersects(st, bb) %>% lengths > 0, c('STATEFP', 'NAME', 'STUSPS') ]
places <- lapply(st$STUSPS, tigris::places)
places <- places %>%
bind_rows() %>%
filter(str_detect(NAMELSAD, 'city')) %>%
dplyr::select(STATEFP, NAME) %>%
sf::st_centroid() %>%
st_transform(4326)
places <- places %>%
rename(CITY = NAME) %>%
left_join(., st %>%
st_drop_geometry(), by = 'STATEFP')
places <- places[st_intersects(places, bb) %>% lengths > 0, ]
places <- st_set_precision(places, precision=10^3)
st_write(places, 'places.shp') # this will drop location coordinates to 3 decimal places.
places <- st_read('places.shp')
View(places)
places <- select(places, -STUSPS, -STATEFP)
places <- select(places, -STUSPS, -STATEFP, STATE = NAME)
places <- st_read('places.shp')
places <- select(places, -STUSPS, -STATEFP, STATE = NAME)
places <- select(places, -STUSPS, -STATEFP, STATE = NAME) %>%
rowid_to_column("ID")
places <- st_read('places.shp')
places <- select(places, -STUSPS, -STATEFP, STATE = NAME) %>%
rowid_to_column("ID")
usethis::use_data(places)
rm(places)
google_towns <- select(places, -STUSPS, -STATEFP, STATE = NAME) %>%
rowid_to_column("ID")
bound <- data.frame(
y = c(30, 30, 50, 50, 30),
x = c(-85, -125, -125, -85, -85)
) %>%
st_as_sf(coords = c('x', 'y'), crs = 4326) %>%
st_bbox() %>%
st_as_sfc()
bb_vals <- c(-125, -85, 30, 50)
w_illi <- ext(bb_vals)
bb <- pgirmess::bbox2sf(w = bb_vals[1], e = bb_vals[2],
s = bb_vals[3], n = bb_vals[4]) %>%
st_as_sf()
st <- tigris::states() %>%
st_transform(4326)
st <- st[st_intersects(st, bb) %>% lengths > 0, c('STATEFP', 'NAME', 'STUSPS') ]
places <- lapply(st$STUSPS, tigris::places)
places <- places %>%
bind_rows() %>%
filter(str_detect(NAMELSAD, 'city')) %>%
dplyr::select(STATEFP, NAME) %>%
sf::st_centroid() %>%
st_transform(4326)
places <- places %>%
rename(CITY = NAME) %>%
left_join(., st %>%
st_drop_geometry(), by = 'STATEFP')
places <- places[st_intersects(places, bb) %>% lengths > 0, ]
places <- st_set_precision(places, precision=10^3)
st_write(places, 'places.shp') # this will drop location coordinates to 3 decimal places.
st_write(places, 'places.shp', append = F) # this will drop location coordinates to 3 decimal places.
places <- st_read('places.shp')
google_towns <- select(places, -STUSPS, -STATEFP, STATE = NAME) %>%
rowid_to_column("ID")
usethis::use_data(google_towns)
rm(places, google_towns)
#' sites <- data.frame(
#'  longitude_dd = runif(15, min = -120, max = -100),
#'  latitude_dd = runif(15, min = 35, max = 48)
#'  ) |>
#'  sf::st_as_sf(coords = c('longitude_dd', 'latitude_dd'), crs = 4326)
#'
#' head(sites)
#' distaze_results <- distAZE(sites, path = '/hdd/Barneby_Lives-dev/geodata/places') # takes some time
#' head(distaze_results)
#' @export
site_writer <- function(x, path){
gnis_places <- sf::st_read(file.path(path, 'places.shp'), quiet = T)
nf <- sf::st_nearest_feature(sites, gnis_places)
sites <- x |>
dplyr::mutate(sf::st_drop_geometry(gnis_places[nf, 'ID']),
.before = geometry)
locality <- sf::st_drop_geometry(sites)
locality <- locality[1, 'ID']
focal <- gnis_places[grep(locality, gnis_places$ID), ]
location_from <- sf::st_centroid(focal)
location_from <- sf::st_transform(location_from, 5070)
x_planar <- sf::st_transform(x, 5070)
distances <- sf::st_distance(location_from, x_planar, which = 'Euclidean')
place <- data.frame('Place' =  st_drop_geometry(gnis_places[x$ID, 'fetr_nm']))
azy <- nngeo::st_azimuth(
location_from,
x_planar
)
distances <- data.frame(
sf::st_drop_geometry(x),
Distance = round(as.numeric(distances / 1609.34), -1),
Azimuth = round(as.numeric(azy), 0),
Place = place
)  |>
dplyr::mutate(Site = if_else(
Distance < 100, paste0('At ', fetr_nm, '.'),
paste0(Distance, 'm', ' at ', format_degree(Azimuth), ' from ', fetr_nm, '.')),
Site = stringr::str_replace(Site, '\\..$', '.')) |>
dplyr::select(-any_of(c('Distance', 'Azimuth', 'Place', 'ID', 'fetr_nm')))
out <- dplyr::bind_cols(x, distances) |>
dplyr::relocate(Site, .before = geometry) |>
dplyr::select(-any_of(c('ID')))
}
#' check that genera and specific epithets are spelled (almost) correctly
#'
#' @description this function attempts to verify the spelling of a user submitted taxonomic name. If necessary it will proceed step-wise by name pieces attempting to place them.
#' @param x a vector of species names
#' @param path a path to a folder containing the taxonomic data.
#' @examples
#' names_vec <- c('Astagalus purshii', 'Linnaeus borealius', 'Heliumorus multifora')
#' spelling <- spell_check(names_vec, path = '../taxonomic_data')
#' spelling
#' @export
spell_check <- function(x, path){
sppLKPtab <- read.csv(file.path(path, 'species_lookup_table.csv'))
epiLKPtab <- read.csv(file.path(path, 'epithet_lookup_table.csv'))
genLKPtab <- read.csv(file.path(path, 'genus_lookup_table.csv'))
pieces <- unlist(stringr::str_split(x, pattern = " "))
genus <- pieces[1] ; species <- pieces[2]
binom <- paste(genus, species)
# infra species should be found without much hassle due to their length
if(length(pieces) == 4){
infras <- na.omit(epiLKPtab)
full_name <- paste(genus, species,
stringr::str_replace(pieces[3], 'ssp\\.|ssp', 'subsp.'), pieces[4])
if (any(grep( x = infras$scientificName, pattern = full_name, fixed = T))) {
return(data.frame(Query = x, Result = full_name, Match = 'exact'))
} else {
infras <- na.omit(epiLKPtab)
full_name <- paste(genus, species,
stringr::str_replace(pieces[3], 'ssp\\.|ssp', 'subsp.'), pieces[4])
infraspecies_name <-
infras[which.min(adist(full_name, infras$scientificName)), 'scientificName'] |> as.character()
return(data.frame(Query = x, Result = infraspecies_name, Match = 'fuzzy'))
}
# species can become difficult due to their short  names, e.g. 'Poa annua'
} else {
if (any(grep( x = sppLKPtab$scientificName, pattern = binom, fixed = T))) {
return(data.frame(Query = x, Result = x, Match = 'exact'))
} else{
# try and determine which piece is incorrect.
# subset datasets to query each name component separately
genus2char <- stringr::str_extract(genus, '[A-Z][a-z]{1}')
species3char <- stringr::str_extract(species, '[a-z]{3}')
gen_strings <-
dplyr::filter(genLKPtab, .data$Grp == genus2char) |> dplyr::pull(strings)
spe_strings <-
dplyr::filter(sppLKPtab, .data$Grp == species3char) |> dplyr::pull(strings)
# check to see if both genus and species are clean
if (any(grep(x = gen_strings, pattern = paste0('^', genus, '$')))) {
clean_genus_Tag <- genus
} else {
possible_genus_Tag <-
gen_strings[which.min(adist(genus, gen_strings))]
}
# is species clean
if (any(grep(x = spe_strings, pattern = paste0('^', species, '$')))) {
clean_species_Tag <- species
} else {
possible_species_Tag <-
spe_strings[which.min(adist(species, spe_strings))]
}
# if both the genus and species name are present, we could be missing it from the DB
if (exists('clean_genus_Tag') & exists ('clean_species_Tag'))
{
return(data.frame(
Query = x, Result = binom, Match = 'Suspected missing from ref DB'))
} else { # if one is not clean search them with the 'cleaned' up versions
combos <- ls()[grep(ls(), pattern = 'Tag')]
search_q <-
combos[c(grep(combos, pattern = 'genus'),
grep(combos, pattern = 'species'))]
search_nom <- paste(unlist(mget(search_q)), collapse = " ")
if (any(grep(x = epiLKPtab$scientificName, pattern = search_nom, fixed = T))) {
return(data.frame(Query = x, Result = search_nom, Match = 'fuzzy'))
} else{
possible_binomial <-
epiLKPtab[which.min(adist(search_nom, epiLKPtab$scientificName)), 'scientificName'] |>
as.character()
return(data.frame(Query = x, Binomial = possible_binomial, Match = 'fuzzy'))
}
}
}
}
}
abbrevs <- read.csv('/hdd/Barneby_Lives-dev/taxonomic_data_raw/ipni_author_abbreviations.csv')
trailed <- vector(mode = 'character', length = nrow(abbrevs))
trailed[grep('\\.$', abbrevs$author_abbrevation)] <- '.'
abbrevs_spaced <- sub('\\.$', '', abbrevs$author_abbrevation) # remove the trailing periods
abbrevs_notrail <- sub('\\.(?!.*\\.)', ". ",
abbrevs_spaced, perl = T) # identify the last period in the name, and add a space after it
abbrevs <- paste0(abbrevs_notrail, trailed)
#usethis::use_data(abbrevs)
write.csv(abbrevs, row.names = F, '../taxonomic_data/abbrevs.csv')
#usethis::use_data(abbrevs)
write.csv(abbrevs, row.names = F, '/hdd/Barneby_Lives-dev/taxonomic_data/abbrevs.csv')
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
usethis::use_rmarkdown_template(
template_name = "Template Name",
template_dir = NULL,
template_description = "A description of the template",
template_create_dir = FALSE
)
